{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import  Dataset,DataLoader,SubsetRandomSampler,random_split\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "import pandas as pd\n",
    "import os\n",
    "from PIL import Image"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA PREPARE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORT DATA"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Datasets\n",
    "\n",
    " - Trainning dataset: https://cloud.ipb.pt/f/657d534db56645059905/?dl=1\n",
    " - Evaluate dataset: https://cloud.ipb.pt/f/27e4d3ac75d2405aa770/?dl=1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dataset dispositions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformation functions and hyperparams\n",
    "\n",
    "Aqui optaremos por duas transformações de imagens de treinamento, em modelos como AlexNet, VGG que trabalham com imagens de escala maior usaremos *227x227 pixels* para treinamento do AlexNet.\n",
    "\n",
    "Outra abordagem será utilizar imagens de menor escala, para um rede neural menor de desenvolvimento próprio, baseado em outros notebooks e estudos relacionados, para esse modelo será utilizado amostras de imagens com a escala de *64x64 pixels*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = (\"1\",\"0\")\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((64,64)),\n",
    "    # transforms.Resize((224,224)),\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.ImageFolder('./train', transform=train_transform)\n",
    "\n",
    "train_size = int(0.9 * len(train_dataset))\n",
    "test_size = len(train_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(train_dataset, [train_size, test_size])\n",
    "\n",
    "# DataLoaders for train and validation\n",
    "train_loader = DataLoader(train_dataset,  shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Models Architecture"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Our CNN architecture to 64x64px image input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionNeuralNetwork(nn.Module):\n",
    "    def __init__(self,num_classes=2):\n",
    "        super(ConvolutionNeuralNetwork, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=24, kernel_size=5, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(24)\n",
    "        self.conv2 = nn.Conv2d(in_channels=24, out_channels=24, kernel_size=5, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(24)\n",
    "\n",
    "        self.pool1 = nn.MaxPool2d(2,2)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(in_channels=24, out_channels=48, kernel_size=5, stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(48)\n",
    "        self.conv4 = nn.Conv2d(in_channels=48, out_channels=48, kernel_size=5, stride=1, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(48)\n",
    "\n",
    "        self.pool2 = nn.MaxPool2d(2,2)\n",
    "        self.drop1=nn.Dropout(p=0.2)\n",
    "\n",
    "        self.conv5 = nn.Conv2d(in_channels=48, out_channels=96, kernel_size=5, stride=1, padding=1)\n",
    "        self.bn5 = nn.BatchNorm2d(96)\n",
    "        self.conv6 = nn.Conv2d(in_channels=96, out_channels=96, kernel_size=4, stride=1, padding=1)\n",
    "        self.bn6 = nn.BatchNorm2d(96)\n",
    "\n",
    "        self.pool3 = nn.MaxPool2d(2,2)\n",
    "\n",
    "        self.fc1 = nn.Linear(96*5*5, 192)\n",
    "        self.drop2=nn.Dropout(p=0.2)\n",
    "\n",
    "        self.fc2 = nn.Linear(192, 96)\n",
    "        self.fc3 = nn.Linear(96, num_classes)\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = F.relu(self.bn1(self.conv1(input)))      \n",
    "        output = F.relu(self.bn2(self.conv2(output)))     \n",
    "        output = self.pool1(output)  \n",
    "\n",
    "        output = F.relu(self.bn3(self.conv3(output)))      \n",
    "        output = F.relu(self.bn4(self.conv4(output)))     \n",
    "        output = self.pool2(output)      \n",
    "        output = self.drop1(output)  \n",
    "\n",
    "        output = F.relu(self.bn5(self.conv5(output)))      \n",
    "        output = F.relu(self.bn6(self.conv6(output)))  \n",
    "        output = self.pool3(output)  \n",
    "\n",
    "        output = output.reshape(output.size(0),-1)\n",
    "\n",
    "        output = F.relu(self.fc1(output))\n",
    "        output = self.drop2(output)\n",
    "        output = F.relu(self.fc2(output))\n",
    "        output = self.fc3(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alex Net architecture to 277x277px image input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNet(nn.Module):\n",
    "    def __init__(self,num_classes=2):\n",
    "        super(AlexNet, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=96, kernel_size=10, stride=4, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(96)\n",
    "        self.pool1= nn.MaxPool2d(3,2)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels=96, out_channels=256, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn2 = nn.BatchNorm2d(256)\n",
    "        self.pool2 = nn.MaxPool2d(3,2)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(in_channels=256, out_channels=384, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(384)\n",
    "\n",
    "        self.conv4 = nn.Conv2d(in_channels=384, out_channels=384, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(384)\n",
    "\n",
    "        self.conv5 = nn.Conv2d(in_channels=384, out_channels=256, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn5 = nn.BatchNorm2d(256)\n",
    "        self.pool3 = nn.MaxPool2d(3,2)\n",
    "\n",
    "        self.fc1 = nn.Linear(9216, 4096)\n",
    "        self.fc2 = nn.Linear(4096, 4096)\n",
    "        self.fc3 = nn.Linear(4096, num_classes)\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = F.relu(self.bn1(self.conv1(input)))   \n",
    "        output = self.pool1(output)     \n",
    "        output = F.relu(self.bn2(self.conv2(output)))     \n",
    "        output = self.pool2(output)    \n",
    "\n",
    "        output = F.relu(self.bn3(self.conv3(output)))     \n",
    "        output = F.relu(self.bn4(self.conv4(output)))   \n",
    "        output = F.relu(self.bn5(self.conv5(output))) \n",
    "        output = self.pool3(output)   \n",
    "\n",
    "        output = output.reshape(output.size(0),-1)\n",
    "\n",
    "        output = F.relu(self.fc1(output))\n",
    "        output = F.relu(self.fc2(output))\n",
    "        output = self.fc3(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Model, loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvolutionNeuralNetwork()\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveModel():\n",
    "    torch.save(model.state_dict(), \"apurated_model_mycnn.pth\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Validation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10):\n",
    "    device = torch.device(\"cpu\")\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        train_correct = 0\n",
    "        for images, labels in train_loader:\n",
    "            images,labels = images.to(device),labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            scores, predictions = torch.max(outputs.data, 1)\n",
    "            train_correct += (predictions == labels).sum().item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader):.4f}, AVG Training Acc {train_correct/len(train_loader):.4f}\")\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images,labels = images.to(device),labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                scores, predictions = torch.max(outputs.data,1)\n",
    "                val_correct+=(predictions == labels).sum().item()\n",
    "\n",
    "        print(f\"Validation Loss: {val_loss/len(val_loader):.4f}, AVG Validation Acc {val_correct/len(val_loader):.4f}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model,device,dataloader):\n",
    "    train_loss,train_correct=0.0,0\n",
    "    model.train()\n",
    "    for images, labels in dataloader:\n",
    "\n",
    "        images,labels = images.to(device),labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(images)\n",
    "        loss = loss_fn(output,labels)\n",
    "\n",
    "        loss.backward()\n",
    "        # accelerator.backward(loss)\n",
    "\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * images.size(0)\n",
    "        scores, predictions = torch.max(output.data, 1)\n",
    "        train_correct += (predictions == labels).sum().item()\n",
    "\n",
    "    return train_loss,train_correct\n",
    "  \n",
    "def valid_epoch(model,device,dataloader):\n",
    "    valid_loss, val_correct = 0.0, 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "\n",
    "            images,labels = images.to(device),labels.to(device)\n",
    "            output = model(images)\n",
    "            loss=loss_fn(output,labels)\n",
    "            \n",
    "            valid_loss+=loss.item()*images.size(0)\n",
    "            scores, predictions = torch.max(output.data,1)\n",
    "            val_correct+=(predictions == labels).sum().item()\n",
    "\n",
    "    return valid_loss,val_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_epochs):\n",
    "    history = {'train_loss': [], 'test_loss': [],'train_acc':[],'test_acc':[]}\n",
    "    best_accuracy = 0.0\n",
    "\n",
    "    model.to(device)\n",
    "    \n",
    "\n",
    "    test_sampler = SubsetRandomSampler(val_idx)\n",
    "    train_samples =  SubsetRandomSampler(train_idx)\n",
    "\n",
    "    test_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=test_sampler)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=train_samples)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss, train_correct=train_epoch(model,device,train_loader)\n",
    "        test_loss, test_correct=valid_epoch(model,device,test_loader)\n",
    "\n",
    "        train_loss = train_loss / len(train_loader.sampler)\n",
    "        train_acc = train_correct / len(train_loader.sampler) * 100\n",
    "\n",
    "        test_loss = test_loss / len(test_loader.sampler)\n",
    "        test_acc = test_correct / len(test_loader.sampler) * 100\n",
    "\n",
    "        print(\"Epoch:{}/{} AVG Training Loss:{:.3f} AVG Test Loss:{:.3f} AVG Training Acc {:.2f} % AVG Test Acc {:.2f} %\".format(epoch + 1,\n",
    "                                                                                                                num_epochs,\n",
    "                                                                                                                train_loss,\n",
    "                                                                                                                test_loss,\n",
    "                                                                                                                train_acc,\n",
    "                                                                                                                test_acc))\n",
    "        if train_acc > best_accuracy:\n",
    "            saveModel()\n",
    "            best_accuracy = train_acc\n",
    "            print(\"Best Accuracy:{} %\".format(best_accuracy))\n",
    "\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['test_loss'].append(test_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['test_acc'].append(test_acc)   \n",
    "\n",
    "    df_history = pd.DataFrame(data=history)\n",
    "    df_history.to_csv(\"historic_mycnn.csv\", encoding='utf-8', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.4534, AVG Training Acc 0.8133\n",
      "Validation Loss: 0.7838, AVG Validation Acc 0.8846\n",
      "Epoch 2/10, Loss: 0.4171, AVG Training Acc 0.8533\n",
      "Validation Loss: 0.1333, AVG Validation Acc 0.9231\n",
      "Epoch 3/10, Loss: 0.3279, AVG Training Acc 0.8578\n",
      "Validation Loss: 0.1875, AVG Validation Acc 1.0000\n",
      "Epoch 4/10, Loss: 0.2956, AVG Training Acc 0.8889\n",
      "Validation Loss: 0.2856, AVG Validation Acc 0.8462\n",
      "Epoch 5/10, Loss: 0.2633, AVG Training Acc 0.9244\n",
      "Validation Loss: 0.1357, AVG Validation Acc 1.0000\n",
      "Epoch 6/10, Loss: 0.1948, AVG Training Acc 0.9556\n",
      "Validation Loss: 0.1481, AVG Validation Acc 0.9615\n",
      "Epoch 7/10, Loss: 0.1681, AVG Training Acc 0.9422\n",
      "Validation Loss: 0.0485, AVG Validation Acc 0.9615\n",
      "Epoch 8/10, Loss: 0.1754, AVG Training Acc 0.9378\n",
      "Validation Loss: 0.1314, AVG Validation Acc 1.0000\n",
      "Epoch 9/10, Loss: 0.2263, AVG Training Acc 0.8978\n",
      "Validation Loss: 0.1027, AVG Validation Acc 1.0000\n",
      "Epoch 10/10, Loss: 0.1671, AVG Training Acc 0.9200\n",
      "Validation Loss: 0.0806, AVG Validation Acc 0.9615\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "trained_model = train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10)\n",
    "print('Finished Training')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0)]\n",
      "Precision: 1.0000\n",
      "Recall: 0.9231\n",
      "F1-Score: 0.9600\n",
      "Final F1-Score: 0.9600\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, val_loader):\n",
    "    model.eval()\n",
    "    device = torch.device('cpu')\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.numpy()  # Convert labels to CPU numpy array\n",
    "            outputs = torch.sigmoid(model(images))\n",
    "            if outputs.size(1) > 1:  # Caso haja múltiplas classes\n",
    "                preds = torch.argmax(outputs, dim=1)  # Seleciona a classe com maior probabilidade\n",
    "            else:\n",
    "                preds = (outputs > 0.5).int().numpy()\n",
    "\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels)\n",
    "\n",
    "    print(all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds)\n",
    "    recall = recall_score(all_labels, all_preds)\n",
    "\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1-Score: {f1:.4f}\")\n",
    "\n",
    "    return f1\n",
    "\n",
    "f1 = evaluate_model(trained_model, val_loader)\n",
    "print(f\"Final F1-Score: {f1:.4f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teste para arquitetura AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to result.txt\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),  # Ajuste o tamanho conforme necessário\n",
    "    transforms.ToTensor(),          # Converte as imagens para tensores\n",
    "])\n",
    "\n",
    "class ImageFolderEval(Dataset):\n",
    "    def __init__(self, directory, transform=None):\n",
    "        self.directory = directory\n",
    "        self.transform = transform\n",
    "        # Listar todas as imagens no diretório\n",
    "        self.images = [f for f in os.listdir(directory) if f.endswith(('jpg', 'jpeg', 'png'))]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.directory, self.images[idx])\n",
    "        image = Image.open(img_path).convert('RGB')  # Abrir a imagem\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, self.images[idx]  \n",
    "\n",
    "# Carregar as imagens do diretório './eval'\n",
    "eval_dir = './eval'\n",
    "eval_dataset = ImageFolderEval(eval_dir, transform=transform)\n",
    "eval_loader = DataLoader(eval_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "result_file = \"result.txt\"\n",
    "trained_model.eval()\n",
    "with open(result_file, 'w') as f:\n",
    "    for i, (images, img_names) in enumerate(eval_loader):\n",
    "        images = images.to(torch.device('cpu'))\n",
    "\n",
    "        # Fazer a previsão com o modelo\n",
    "        outputs = torch.sigmoid(trained_model(images))\n",
    "\n",
    "        # Se o modelo for multi-classe ou multi-rótulo, use o índice da classe com maior probabilidade\n",
    "        if outputs.size(1) > 1:  # Caso haja múltiplas classes\n",
    "            preds = torch.argmax(outputs, dim=1)  # Seleciona a classe com maior probabilidade\n",
    "        else:\n",
    "            preds = (outputs > 0.5).int()  # Para um único valor de saída (classificação binária)\n",
    "\n",
    "        # Iterando sobre cada previsão no batch\n",
    "        for j, (pred, img_name) in enumerate(zip(preds, img_names)):\n",
    "            f.write(f\"{img_name} {pred.item()}\\n\")\n",
    "\n",
    "print(f\"Predictions saved to {result_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

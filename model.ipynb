{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import  DataLoader,SubsetRandomSampler,random_split\n",
    "from sklearn.model_selection import KFold\n",
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA PREPARE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORT DATA"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Datasets\n",
    "\n",
    " - Trainning dataset: https://cloud.ipb.pt/f/657d534db56645059905/?dl=1\n",
    " - Evaluate dataset: https://cloud.ipb.pt/f/27e4d3ac75d2405aa770/?dl=1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dataset dispositions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformation functions and hyperparams\n",
    "\n",
    "Aqui optaremos por duas transformações de imagens de treinamento, em modelos como AlexNet, VGG que trabalham com imagens de escala maior usaremos *227x227 pixels* para treinamento do AlexNet.\n",
    "\n",
    "Outra abordagem será utilizar imagens de menor escala, para um rede neural menor de desenvolvimento próprio, baseado em outros notebooks e estudos relacionados, para esse modelo será utilizado amostras de imagens com a escala de *64x64 pixels*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "classes = (1,0)\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((64,64)),\n",
    "    # transforms.Resize((224,224)),\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.ImageFolder('./train', transform=train_transform)\n",
    "\n",
    "train_size = int(0.9 * len(train_dataset))\n",
    "test_size = len(train_dataset) - train_size\n",
    "train_dataset_split, prediction_dataset = random_split(train_dataset, [train_size, test_size])\n",
    "\n",
    "# DataLoaders for train and validation\n",
    "train_loader = DataLoader(train_dataset_split, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(train_dataset_split, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# After evaluation, use the prediction dataset\n",
    "prediction_loader = DataLoader(prediction_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Models Architecture"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Our CNN architecture to 64x64px image input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionNeuralNetwork(nn.Module):\n",
    "    def __init__(self,num_classes=7):\n",
    "        super(ConvolutionNeuralNetwork, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=24, kernel_size=5, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(24)\n",
    "        self.conv2 = nn.Conv2d(in_channels=24, out_channels=24, kernel_size=5, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(24)\n",
    "\n",
    "        self.pool1 = nn.MaxPool2d(2,2)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(in_channels=24, out_channels=48, kernel_size=5, stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(48)\n",
    "        self.conv4 = nn.Conv2d(in_channels=48, out_channels=48, kernel_size=5, stride=1, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(48)\n",
    "\n",
    "        self.pool2 = nn.MaxPool2d(2,2)\n",
    "        self.drop1=nn.Dropout(p=0.2)\n",
    "\n",
    "        self.conv5 = nn.Conv2d(in_channels=48, out_channels=96, kernel_size=5, stride=1, padding=1)\n",
    "        self.bn5 = nn.BatchNorm2d(96)\n",
    "        self.conv6 = nn.Conv2d(in_channels=96, out_channels=96, kernel_size=4, stride=1, padding=1)\n",
    "        self.bn6 = nn.BatchNorm2d(96)\n",
    "\n",
    "        self.pool3 = nn.MaxPool2d(2,2)\n",
    "\n",
    "        self.fc1 = nn.Linear(96*5*5, 192)\n",
    "        self.drop2=nn.Dropout(p=0.2)\n",
    "\n",
    "        self.fc2 = nn.Linear(192, 96)\n",
    "        self.fc3 = nn.Linear(96, num_classes)\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = F.relu(self.bn1(self.conv1(input)))      \n",
    "        output = F.relu(self.bn2(self.conv2(output)))     \n",
    "        output = self.pool1(output)  \n",
    "\n",
    "        output = F.relu(self.bn3(self.conv3(output)))      \n",
    "        output = F.relu(self.bn4(self.conv4(output)))     \n",
    "        output = self.pool2(output)      \n",
    "        output = self.drop1(output)  \n",
    "\n",
    "        output = F.relu(self.bn5(self.conv5(output)))      \n",
    "        output = F.relu(self.bn6(self.conv6(output)))  \n",
    "        output = self.pool3(output)  \n",
    "\n",
    "        output = output.reshape(output.size(0),-1)\n",
    "\n",
    "        output = F.relu(self.fc1(output))\n",
    "        output = self.drop2(output)\n",
    "        output = F.relu(self.fc2(output))\n",
    "        output = self.fc3(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alex Net architecture to 277x277px image input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNet(nn.Module):\n",
    "    def __init__(self,num_classes=7):\n",
    "        super(AlexNet, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=96, kernel_size=10, stride=4, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(96)\n",
    "        self.pool1= nn.MaxPool2d(3,2)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels=96, out_channels=256, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn2 = nn.BatchNorm2d(256)\n",
    "        self.pool2 = nn.MaxPool2d(3,2)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(in_channels=256, out_channels=384, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(384)\n",
    "\n",
    "        self.conv4 = nn.Conv2d(in_channels=384, out_channels=384, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(384)\n",
    "\n",
    "        self.conv5 = nn.Conv2d(in_channels=384, out_channels=256, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn5 = nn.BatchNorm2d(256)\n",
    "        self.pool3 = nn.MaxPool2d(3,2)\n",
    "\n",
    "        self.fc1 = nn.Linear(9216, 4096)\n",
    "        self.fc2 = nn.Linear(4096, 4096)\n",
    "        self.fc3 = nn.Linear(4096, num_classes)\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = F.relu(self.bn1(self.conv1(input)))   \n",
    "        output = self.pool1(output)     \n",
    "        output = F.relu(self.bn2(self.conv2(output)))     \n",
    "        output = self.pool2(output)    \n",
    "\n",
    "        output = F.relu(self.bn3(self.conv3(output)))     \n",
    "        output = F.relu(self.bn4(self.conv4(output)))   \n",
    "        output = F.relu(self.bn5(self.conv5(output))) \n",
    "        output = self.pool3(output)   \n",
    "\n",
    "        output = output.reshape(output.size(0),-1)\n",
    "\n",
    "        output = F.relu(self.fc1(output))\n",
    "        output = F.relu(self.fc2(output))\n",
    "        output = self.fc3(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvolutionNeuralNetwork()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runing on: cpu\n"
     ]
    }
   ],
   "source": [
    "# accelerator = Accelerator()\n",
    "# device = accelerator.device\n",
    "# Define your execution device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Runing on: \"+ (\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "learning_rate = 0.05\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=learning_rate,  weight_decay = 0.01)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveModel():\n",
    "    torch.save(model.state_dict(), \"apurated_model_mycnn.pth\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Validation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model,device,dataloader):\n",
    "    train_loss,train_correct=0.0,0\n",
    "    model.train()\n",
    "    for images, labels in dataloader:\n",
    "\n",
    "        images,labels = images.to(device),labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(images)\n",
    "        loss = loss_fn(output,labels)\n",
    "\n",
    "        loss.backward()\n",
    "        # accelerator.backward(loss)\n",
    "\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * images.size(0)\n",
    "        scores, predictions = torch.max(output.data, 1)\n",
    "        train_correct += (predictions == labels).sum().item()\n",
    "\n",
    "    return train_loss,train_correct\n",
    "  \n",
    "def valid_epoch(model,device,dataloader):\n",
    "    valid_loss, val_correct = 0.0, 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "\n",
    "            images,labels = images.to(device),labels.to(device)\n",
    "            output = model(images)\n",
    "            loss=loss_fn(output,labels)\n",
    "            \n",
    "            valid_loss+=loss.item()*images.size(0)\n",
    "            scores, predictions = torch.max(output.data,1)\n",
    "            val_correct+=(predictions == labels).sum().item()\n",
    "\n",
    "    return valid_loss,val_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_epochs):\n",
    "    history = {'train_loss': [], 'test_loss': [],'train_acc':[],'test_acc':[]}\n",
    "    best_accuracy = 0.0\n",
    "\n",
    "    model.to(device)\n",
    "    \n",
    "\n",
    "    test_sampler = SubsetRandomSampler(val_idx)\n",
    "    train_samples =  SubsetRandomSampler(train_idx)\n",
    "\n",
    "    test_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=test_sampler)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=train_samples)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss, train_correct=train_epoch(model,device,train_loader)\n",
    "        test_loss, test_correct=valid_epoch(model,device,test_loader)\n",
    "\n",
    "        train_loss = train_loss / len(train_loader.sampler)\n",
    "        train_acc = train_correct / len(train_loader.sampler) * 100\n",
    "\n",
    "        test_loss = test_loss / len(test_loader.sampler)\n",
    "        test_acc = test_correct / len(test_loader.sampler) * 100\n",
    "\n",
    "        print(\"Epoch:{}/{} AVG Training Loss:{:.3f} AVG Test Loss:{:.3f} AVG Training Acc {:.2f} % AVG Test Acc {:.2f} %\".format(epoch + 1,\n",
    "                                                                                                                num_epochs,\n",
    "                                                                                                                train_loss,\n",
    "                                                                                                                test_loss,\n",
    "                                                                                                                train_acc,\n",
    "                                                                                                                test_acc))\n",
    "        if train_acc > best_accuracy:\n",
    "            saveModel()\n",
    "            best_accuracy = train_acc\n",
    "            print(\"Best Accuracy:{} %\".format(best_accuracy))\n",
    "\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['test_loss'].append(test_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['test_acc'].append(test_acc)   \n",
    "\n",
    "    df_history = pd.DataFrame(data=history)\n",
    "    df_history.to_csv(\"historic_mycnn.csv\", encoding='utf-8', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'allocated_bytes.all.current'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmemory_summary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mabbreviated\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/si-covid-deep-learning-classifier/.venv/lib/python3.12/site-packages/torch/cuda/memory.py:587\u001b[0m, in \u001b[0;36mmemory_summary\u001b[0;34m(device, abbreviated)\u001b[0m\n\u001b[1;32m    584\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m submetric_key, submetric_name \u001b[38;5;129;01min\u001b[39;00m submetrics:\n\u001b[1;32m    585\u001b[0m     prefix \u001b[38;5;241m=\u001b[39m metric_key \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m submetric_key \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 587\u001b[0m     current \u001b[38;5;241m=\u001b[39m \u001b[43mstats\u001b[49m\u001b[43m[\u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcurrent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    588\u001b[0m     peak \u001b[38;5;241m=\u001b[39m stats[prefix \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpeak\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    589\u001b[0m     allocated \u001b[38;5;241m=\u001b[39m stats[prefix \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallocated\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'allocated_bytes.all.current'"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "torch.cuda.memory_summary(device=None, abbreviated=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Epoch:1/5 AVG Training Loss:0.027 AVG Test Loss:0.107 AVG Training Acc 99.00 % AVG Test Acc 98.04 %\n",
      "Best Accuracy:99.0 %\n",
      "Epoch:2/5 AVG Training Loss:0.018 AVG Test Loss:0.121 AVG Training Acc 99.00 % AVG Test Acc 98.04 %\n",
      "Epoch:3/5 AVG Training Loss:0.006 AVG Test Loss:0.124 AVG Training Acc 100.00 % AVG Test Acc 98.04 %\n",
      "Best Accuracy:100.0 %\n",
      "Epoch:4/5 AVG Training Loss:0.005 AVG Test Loss:0.101 AVG Training Acc 100.00 % AVG Test Acc 98.04 %\n",
      "Epoch:5/5 AVG Training Loss:0.014 AVG Test Loss:0.271 AVG Training Acc 99.50 % AVG Test Acc 94.12 %\n",
      "Fold 2\n",
      "Epoch:1/5 AVG Training Loss:0.078 AVG Test Loss:0.009 AVG Training Acc 98.01 % AVG Test Acc 100.00 %\n",
      "Epoch:2/5 AVG Training Loss:0.020 AVG Test Loss:0.095 AVG Training Acc 99.00 % AVG Test Acc 96.00 %\n",
      "Epoch:3/5 AVG Training Loss:0.043 AVG Test Loss:0.083 AVG Training Acc 98.01 % AVG Test Acc 96.00 %\n",
      "Epoch:4/5 AVG Training Loss:0.017 AVG Test Loss:0.153 AVG Training Acc 99.00 % AVG Test Acc 96.00 %\n",
      "Epoch:5/5 AVG Training Loss:0.045 AVG Test Loss:0.138 AVG Training Acc 99.50 % AVG Test Acc 96.00 %\n",
      "Fold 3\n",
      "Epoch:1/5 AVG Training Loss:0.112 AVG Test Loss:0.034 AVG Training Acc 96.02 % AVG Test Acc 98.00 %\n",
      "Epoch:2/5 AVG Training Loss:0.052 AVG Test Loss:0.028 AVG Training Acc 98.01 % AVG Test Acc 98.00 %\n",
      "Epoch:3/5 AVG Training Loss:0.053 AVG Test Loss:0.152 AVG Training Acc 99.00 % AVG Test Acc 96.00 %\n",
      "Epoch:4/5 AVG Training Loss:0.043 AVG Test Loss:0.199 AVG Training Acc 98.01 % AVG Test Acc 94.00 %\n",
      "Epoch:5/5 AVG Training Loss:0.109 AVG Test Loss:0.055 AVG Training Acc 97.01 % AVG Test Acc 98.00 %\n",
      "Fold 4\n",
      "Epoch:1/5 AVG Training Loss:0.050 AVG Test Loss:0.056 AVG Training Acc 98.01 % AVG Test Acc 98.00 %\n",
      "Epoch:2/5 AVG Training Loss:0.031 AVG Test Loss:0.034 AVG Training Acc 98.51 % AVG Test Acc 98.00 %\n",
      "Epoch:3/5 AVG Training Loss:0.048 AVG Test Loss:0.024 AVG Training Acc 98.51 % AVG Test Acc 98.00 %\n",
      "Epoch:4/5 AVG Training Loss:0.047 AVG Test Loss:0.082 AVG Training Acc 98.01 % AVG Test Acc 98.00 %\n",
      "Epoch:5/5 AVG Training Loss:0.023 AVG Test Loss:0.015 AVG Training Acc 99.00 % AVG Test Acc 100.00 %\n",
      "Fold 5\n",
      "Epoch:1/5 AVG Training Loss:0.029 AVG Test Loss:0.005 AVG Training Acc 98.51 % AVG Test Acc 100.00 %\n",
      "Epoch:2/5 AVG Training Loss:0.050 AVG Test Loss:0.011 AVG Training Acc 98.51 % AVG Test Acc 100.00 %\n",
      "Epoch:3/5 AVG Training Loss:0.041 AVG Test Loss:0.045 AVG Training Acc 98.01 % AVG Test Acc 98.00 %\n",
      "Epoch:4/5 AVG Training Loss:0.045 AVG Test Loss:0.008 AVG Training Acc 97.51 % AVG Test Acc 100.00 %\n",
      "Epoch:5/5 AVG Training Loss:0.023 AVG Test Loss:0.001 AVG Training Acc 99.00 % AVG Test Acc 100.00 %\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "train(5)\n",
    "print('Finished Training')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Couldn't find any class folder in ./eval.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 8\u001b[0m\n\u001b[1;32m      1\u001b[0m test_transform \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# transforms.Resize((227,227)),\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mResize((\u001b[38;5;241m64\u001b[39m,\u001b[38;5;241m64\u001b[39m)),\n\u001b[1;32m      4\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mGrayscale(num_output_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m),\n\u001b[1;32m      5\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mToTensor(),\n\u001b[1;32m      6\u001b[0m ])\n\u001b[0;32m----> 8\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdatasets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mImageFolder\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./eval\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_transform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m test_loaded \u001b[38;5;241m=\u001b[39m DataLoader(test_dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/Desktop/si-covid-deep-learning-classifier/.venv/lib/python3.12/site-packages/torchvision/datasets/folder.py:328\u001b[0m, in \u001b[0;36mImageFolder.__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file, allow_empty)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    321\u001b[0m     root: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    326\u001b[0m     allow_empty: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    327\u001b[0m ):\n\u001b[0;32m--> 328\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[43m        \u001b[49m\u001b[43mIMG_EXTENSIONS\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mis_valid_file\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget_transform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_transform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_valid_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_valid_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_empty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_empty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    337\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples\n",
      "File \u001b[0;32m~/Desktop/si-covid-deep-learning-classifier/.venv/lib/python3.12/site-packages/torchvision/datasets/folder.py:149\u001b[0m, in \u001b[0;36mDatasetFolder.__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file, allow_empty)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    140\u001b[0m     root: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    146\u001b[0m     allow_empty: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    147\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(root, transform\u001b[38;5;241m=\u001b[39mtransform, target_transform\u001b[38;5;241m=\u001b[39mtarget_transform)\n\u001b[0;32m--> 149\u001b[0m     classes, class_to_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_classes\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    150\u001b[0m     samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_dataset(\n\u001b[1;32m    151\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot,\n\u001b[1;32m    152\u001b[0m         class_to_idx\u001b[38;5;241m=\u001b[39mclass_to_idx,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    155\u001b[0m         allow_empty\u001b[38;5;241m=\u001b[39mallow_empty,\n\u001b[1;32m    156\u001b[0m     )\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader \u001b[38;5;241m=\u001b[39m loader\n",
      "File \u001b[0;32m~/Desktop/si-covid-deep-learning-classifier/.venv/lib/python3.12/site-packages/torchvision/datasets/folder.py:234\u001b[0m, in \u001b[0;36mDatasetFolder.find_classes\u001b[0;34m(self, directory)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfind_classes\u001b[39m(\u001b[38;5;28mself\u001b[39m, directory: Union[\u001b[38;5;28mstr\u001b[39m, Path]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[List[\u001b[38;5;28mstr\u001b[39m], Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m]]:\n\u001b[1;32m    208\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Find the class folders in a dataset structured as follows::\u001b[39;00m\n\u001b[1;32m    209\u001b[0m \n\u001b[1;32m    210\u001b[0m \u001b[38;5;124;03m        directory/\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;124;03m        (Tuple[List[str], Dict[str, int]]): List of all classes and dictionary mapping each class to an index.\u001b[39;00m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind_classes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/si-covid-deep-learning-classifier/.venv/lib/python3.12/site-packages/torchvision/datasets/folder.py:43\u001b[0m, in \u001b[0;36mfind_classes\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m     41\u001b[0m classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(entry\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mscandir(directory) \u001b[38;5;28;01mif\u001b[39;00m entry\u001b[38;5;241m.\u001b[39mis_dir())\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m classes:\n\u001b[0;32m---> 43\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find any class folder in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdirectory\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     45\u001b[0m class_to_idx \u001b[38;5;241m=\u001b[39m {cls_name: i \u001b[38;5;28;01mfor\u001b[39;00m i, cls_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(classes)}\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m classes, class_to_idx\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Couldn't find any class folder in ./eval."
     ]
    }
   ],
   "source": [
    "test_transform = transforms.Compose([\n",
    "    # transforms.Resize((227,227)),\n",
    "    transforms.Resize((64,64)),\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "test_dataset = datasets.ImageFolder('./eval', transform=test_transform)\n",
    "test_loaded = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model):\n",
    "    y_true=[]\n",
    "    y_pred=[]\n",
    "\n",
    "    valid_loss, val_correct = 0.0, 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loaded:\n",
    "            y_true.extend(labels.numpy())\n",
    "\n",
    "            output = model(images)\n",
    "            loss=loss_fn(output,labels)\n",
    "            valid_loss+=loss.item()*images.size(0)\n",
    "            scores, predictions = torch.max(output.data,1)\n",
    "            val_correct+=(predictions == labels).sum().item()\n",
    "\n",
    "            y_pred.extend(predictions.cpu().numpy())\n",
    "\n",
    "    return valid_loss,val_correct, y_pred, y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testBatch(model):\n",
    "    model.eval()\n",
    "    # get batch of images from the test DataLoader  \n",
    "    images, labels = next(iter(test_loaded))\n",
    "   \n",
    "    # show all images as one image grid\n",
    "    img = torchvision.utils.make_grid(images)     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "   \n",
    "    # Show the real labels on the screen \n",
    "    print('Real labels: ', ' '.join('%5s' % classes[labels[j]] \n",
    "                               for j in range(batch_size)))\n",
    "  \n",
    "    with torch.no_grad():\n",
    "        output = model(images)\n",
    "        scores, predictions = torch.max(output.data,1)\n",
    "        \n",
    "        # Let's show the predicted labels on the screen to compare with the real ones\n",
    "        print('Predicted: ', ' '.join('%5s' % classes[predictions[j]] \n",
    "                                for j in range(batch_size)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teste para arquitetura AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = AlexNet()\n",
    "path = \"apurated_model_alex.pth\"\n",
    "model.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_loss,val_correct,y_pred,y_true = test_model(model)\n",
    "\n",
    "valid_loss = valid_loss / len(test_loaded.sampler)\n",
    "val_acc = val_correct / len(test_loaded.sampler) * 100\n",
    "\n",
    "print(\"AVG Test Loss:{:.3f} AVG Test Acc {:.2f} %\".format(valid_loss,val_acc))\n",
    "                                    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "cf_matrix = confusion_matrix(y_true, y_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Mounting the confusion tabel to see error per class\n",
    "\n",
    "A linha indica o valor real da classe e as colunas as predições, então por exemplo, na celula [2][3], quando a classe real era *happy* ele previu 0 imagens como *neutral*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.DataFrame(cf_matrix, index=classes, columns=classes)\n",
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for classlabel in classes:\n",
    "    percent_value =((dataframe[classlabel][classlabel])/100)*100\n",
    "    print(\"Para classe {}, a accurácia foi de {:.2f} %\".format(classlabel,percent_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    " \n",
    "# Create heatmap\n",
    "sns.heatmap(dataframe, annot=True, cbar=None,cmap=\"YlGnBu\",fmt=\"d\")\n",
    " \n",
    "plt.title(\"Confusion Matrix\"), plt.tight_layout()\n",
    " \n",
    "plt.ylabel(\"True Class\"), \n",
    "plt.xlabel(\"Predicted Class\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teste para nossa arquitetura CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvolutionNeuralNetwork()\n",
    "path = \"apurated_model_mycnn.pth\"\n",
    "model.load_state_dict(torch.load(path))\n",
    "\n",
    "valid_loss,val_correct,y_pred,y_true = test_model(model)\n",
    "\n",
    "valid_loss = valid_loss / len(test_loaded.sampler)\n",
    "val_acc = val_correct / len(test_loaded.sampler) * 100\n",
    "\n",
    "print(\"AVG Test Loss:{:.3f} AVG Test Acc {:.2f} %\".format(valid_loss,val_acc))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_matrix = confusion_matrix(y_true, y_pred)\n",
    "dataframe = pd.DataFrame(cf_matrix, index=classes, columns=classes)\n",
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for classlabel in classes:\n",
    "    percent_value =((dataframe[classlabel][classlabel])/100)*100\n",
    "    print(\"Para classe {}, a accurácia foi de {:.2f} %\".format(classlabel,percent_value))\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    " \n",
    "# Create heatmap\n",
    "sns.heatmap(dataframe, annot=True, cbar=None,cmap=\"YlGnBu\",fmt=\"d\")\n",
    " \n",
    "plt.title(\"Confusion Matrix\"), plt.tight_layout()\n",
    " \n",
    "plt.ylabel(\"True Class\"), \n",
    "plt.xlabel(\"Predicted Class\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
